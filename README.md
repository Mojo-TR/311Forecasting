# 311 Detector

## Overview

311 Detector is an end-to-end data analytics and forecasting platform built to analyze Houston 311 service requests. The project ingests raw city data, transforms it into analytics ready datasets, and powers interactive dashboards and time-series forecasts to surface trends, bottlenecks, and future demand.

The system is designed with performance and scalability in mind, using precomputed datasets and modular forecasting logic to support fast, reliable visualizations.

---

## Key Capabilities
- **Data Engineering Pipeline**
    - Ingests large, raw 311 service request files
    - Cleans, normalizes, and enriches records
    - Outputs structured Parquet layers for fast access
- **Interactive Analytics Dashboard**
    - Built with Dash and Plotly
    - Filters by neighborhood, department, division, and category
    - KPI summaries, tables, and trend visualizations
- **Time Series Forecasting**
    - Monthly volume and severity forecasts using Prophet
    - Citywide and neighborhood level projections
    - Confidence intervals and rolling trend extensions
    - Reliability checks (minimum history, error thresholds)
- **Performance First Design**
    - Precomputed forecast and summary tables
    - Avoids heavy computation at request time
    - Modular utilities for reuse and testing

---

## Tech Stack
- **Language:** Python
- **Database:** PostgreSQL
- **Data Processing:** Pandas, NumPy
- **Storage & Caching:** PostgreSQL (system of record), Parquet (precomputed analytics & forecasts)
- **Forecasting:** Prophet
- **Visualization:** Dash, Plotly, Dash Bootstrap Components

---

## Project Structure
```
311Detector/                  # Project root
├── app/
│   ├── assets/               # Static Dash assets
│   ├── pages/                # Dashboard views (multi page Dash app)
│   ├── utils/                # Data loading, forecasting, shared helpers
│   └── app.py                # Dash application entry point
│
├── csv/                      # Exported CSV outputs (optional / local)
├── data/                     # Raw 311 source files (local only, ignored)
├── notebooks/                # Exploration and prototyping notebooks
│
├── precompute/               # Precompute pipeline modules
├── precomputed_data/         # Cached analytics & forecast outputs (Parquet)
│
├── precompute.py             # Runs full precompute pipeline
├── refresh_data.py           # Refreshes data and rebuilds outputs
├── README.md
```
---

## Installation
**Python 3.10+ recommended**

```bash
gitclone https://github.com/Mojo-TR/311Forecasting
cd 311Detector
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

```

---
## Data

### Raw Source Files (Houston 311) (Last Updated January 13th, 2026)

This project is built on the official Houston 311 public service request data.

The raw, unprocessed text files used for ingestion and ETL are available here:

- **Google Drive (raw data folder):**
    
    https://drive.google.com/drive/folders/1AylvJ5GZSWKYIVifinu0HLaRoHAT1IT-?usp=drive_link
    

These files mirror the structure and format of the original City of Houston releases and are used as the input for the data cleaning and transformation pipeline.

---

### Cleaned Export (Processed Dataset)

The cleaned and standardized dataset generated by the pipeline used for analysis, visualization, and forecasting—is available as a single CSV export.

- **Google Drive (view):**
    
    https://drive.google.com/file/d/1O379c5TLYmoanKo9v-he7J7Knj62b76L/view?usp=drive_link
    
- **Last updated:** January 13, 2026
- **Contents:** Normalized columns, parsed timestamps, derived metrics (e.g., resolution time), and category mappings applied.

This export represents the finalized, analysis-ready version of the data used throughout the project.

---

> Notes:
> - This file is generated from the PostgreSQL table after the refresh + upsert pipeline runs.
> - It may be updated periodically; check the file timestamp in Drive.

---

## Google Colab Setup (Optional)

If you're running analysis notebooks in **Google Colab**, install the visualization + modeling libraries:

```python
!pip -q install pandas numpy matplotlib plotly scikit-learn statsmodels prophet pyarrow fastparquet
```

After installing libraries in Colab, restart the runtime:
Runtime → Restart runtime

This is required for Prophet to import correctly.

---

## How to Use the Dataset in Notebooks

### Option A: Google Colab (Recommended)

### 1.) Download the dataset

```python
!wget -q"https://drive.google.com/uc?export=download&id=12wcNjMJwK406nBA-pEQ17nXMWFKFrvGC" -O Houston_311_Export.csv

```

### 2.) Load it into Pandas

```python
import pandasas pd

df = pd.read_csv("Houston_311_Export.csv")
df.head()

```

That’s it. You’re working with the full cleaned dataset.

---

### Option B: Local Python

### 1.) Download

- Click the **Direct download** link in the Data section
- Keep the file as `Houston_311_Export.csv`

### 2.) Load

```python
import pandasas pd

df = pd.read_csv("Houston_311_Export.csv")

```

---

## Dataset Overview

**Each row represents one 311 service request.**

Key columns:

- `CASE NUMBER` — unique identifier
- `CREATED DATE` — request creation timestamp
- `CLOSED DATE` — resolution timestamp (if closed)
- `RESOLUTION_TIME_DAYS` — rounded resolution time
- `NEIGHBORHOOD` — standardized super neighborhood
- `DEPARTMENT`, `DIVISION`
- `CASE TYPE`
- `CATEGORY` — normalized case grouping
- `LATITUDE`, `LONGITUDE`

Dates are timezone naive and parsed for immediate analysis.

---

## Common Examples

### Monthly complaint volume

```python
df["CREATED DATE"] = pd.to_datetime(df["CREATED DATE"])
monthly = (
    df.groupby(df["CREATED DATE"].dt.to_period("M"))
      .size()
      .reset_index(name="count")
)

```

### Average resolution time by category

```python
df.groupby("CATEGORY")["RESOLUTION_TIME_DAYS"].mean().sort_values()

```

### Map ready data

Latitude and longitude are already filtered to Houston boundaries.

---

## Notes

- The dataset is exported **after** database deduplication and upserts
- Data reflects a rolling multi year window
- Missing values are preserved as nulls (not empty strings)

---

## Running the App
```bash
python app/app.py

```

Open your browser to:

```
http://127.0.0.1:8050

```

---

## Data Notes
- Data is sourced from public Houston 311 service request records
- Forecasts are built on **monthly aggregates**
- Low volume or incomplete months are excluded to improve model reliability
- Forecast outputs may differ slightly from raw aggregations due to:
    - Volume thresholds
    - Rolling averages
    - Forecast horizon boundaries

These design choices are intentional and documented to prioritize stability over noise.

## Data Dictionary
A full data dictionary describing metrics, dimensions, and derived fields is available in the dashboard and documented in `docs/data_dictionary.md`.

---

## Forecasting Approach
- Prophet models with yearly seasonality
- Separate pipelines for:
    - **Volume forecasts** (request counts)
    - **Severity forecasts** (resolution time metrics)
- Reliability flags based on:
    - Minimum historical length
    - Error thresholds (MAPE)
- Rolling trends shown for interpretability only (not model inputs)

---

## Forecasting Methodology

### Model
Forecasts are generated using Facebook Prophet with yearly seasonality enabled. The model is selected for its robustness to missing data, ability to capture seasonal patterns, and interpretability.

### Targets
Two primary forecast types are supported:
- **Volume**: monthly count of 311 service requests
- **Severity**: monthly resolution time–based metrics

### Aggregation & Training Data
- Data is aggregated at a monthly frequency
- Incomplete or low volume months are excluded
- Forecasts are generated at citywide and neighborhood levels depending on configuration
- Minimum historical length is required before a forecast is attempted

### Forecast Horizon
- Forecasts extend a fixed number of months into the future
- Historical data and forecasted values are concatenated for visualization continuity

### Reliability Rules
Forecasts are marked unreliable or skipped when:
- Insufficient historical data remains after filtering
- Excessive missing values are present
- Error thresholds (e.g., MAPE) exceed acceptable limits

### Error Metrics & Edge Case Handling

Forecast accuracy is evaluated using Mean Absolute Percentage Error (MAPE), computed on a holdout window of recent historical data when sufficient observations are available.

MAPE is used as a reliability signal rather than an optimization objective.

#### Edge Case Handling
- Forecasts are skipped or marked unreliable when:
  - Actual values contain zeros or near zeros that would inflate MAPE
  - The training series is constant or exhibits insufficient variance
  - Too few observations remain after preprocessing
- In cases where MAPE cannot be computed reliably, forecasts default to an "unreliable" status rather than emitting misleading error values

### Notes on Reproducibility
Exploratory notebooks may produce slightly different results due to alternative preprocessing choices or relaxed filtering. Dashboards display only validated, precomputed forecasts designed for stability and comparability.

---

## Why This Project Matters
This project demonstrates:

- Real world data engineering on messy public datasets
- Scalable dashboard architecture
- Practical forecasting with uncertainty awareness
- Clear separation between data prep, modeling, and presentation

It’s built to mirror how analytics systems are designed in production—not notebooks.

---

## Notes on Forecast Consistency
Forecast outputs in exploratory notebooks may differ slightly from those shown in the dashboard.

This is intentional.

- Notebooks are used for experimentation, diagnostics, and model iteration
- Dashboards rely on validated, precomputed forecast layers designed for stability
- Dashboard forecasts apply stricter rules, including:
  - Minimum data volume thresholds
  - Removal of incomplete or low signal months
  - Fixed forecast horizons
  - Reliability checks (e.g., error thresholds)

As a result, notebook forecasts may explore alternative assumptions or include edge cases that are excluded from production dashboards.

In short: notebooks prioritize exploration, dashboards prioritize reliability.

---

## Onboarding & Operations
Operational workflows for refreshing data, updating logic, and performing QA checks are documented in `docs/onboarding.md`.

---

## Future Improvements
- Scheduled refresh via cron / task runner
- Model comparison (Prophet vs ARIMA)
- Deployment to cloud hosting